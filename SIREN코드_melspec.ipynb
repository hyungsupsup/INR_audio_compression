{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71679dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from asteroid.metrics import get_metrics\n",
    "from asteroid.losses import singlesrc_neg_sisdr, singlesrc_neg_snr\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68be7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "sr = 48000\n",
    "\n",
    "# mel_spectrogram = nn.Sequential(\n",
    "#         AT.MelSpectrogram(sample_rate=sr,\n",
    "#                         n_fft=512,\n",
    "#                         win_length=400,\n",
    "#                         hop_length=160,\n",
    "#                         n_mels=40),\n",
    "#     AT.AmplitudeToDB()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97806c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39664363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mgrid(sidelen, dim=2):\n",
    "#     '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
    "#     sidelen: int\n",
    "#     dim: int'''\n",
    "#     tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
    "#     mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
    "#     mgrid = mgrid.reshape(-1, dim)\n",
    "#     return mgrid\n",
    "\n",
    "def get_mgrid(melspec, dim=2):\n",
    "    coordinates = torch.ones(melspec.shape[1:]).nonzero(as_tuple=False).float()\n",
    "    coordinates = coordinates / (melspec.shape[1] - 1) - 0.5\n",
    "    coordinates *= 2\n",
    "    print(\"coordinates shape : \",coordinates.shape)\n",
    "    return coordinates\n",
    "    \n",
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                             1 / self.in_features)      \n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
    "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return torch.sin(self.omega_0 * self.linear(input))\n",
    "    \n",
    "    def forward_with_intermediate(self, input): \n",
    "        # For visualization of activation distributions\n",
    "        intermediate = self.omega_0 * self.linear(input)\n",
    "        return torch.sin(intermediate), intermediate\n",
    "    \n",
    "    \n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
    "                 first_omega_0=30, hidden_omega_0=30.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = []\n",
    "        self.net.append(SineLayer(in_features, hidden_features, \n",
    "                                  is_first=True, omega_0=first_omega_0))\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
    "                                      is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = nn.Linear(hidden_features, out_features)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
    "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
    "                \n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(SineLayer(hidden_features, out_features, \n",
    "                                      is_first=False, omega_0=hidden_omega_0))\n",
    "        \n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
    "        output = self.net(coords)\n",
    "        return output, coords        \n",
    "\n",
    "    def forward_with_activations(self, coords, retain_grad=False):\n",
    "        '''Returns not only model output, but also intermediate activations.\n",
    "        Only used for visualizing activations later!'''\n",
    "        activations = {}\n",
    "\n",
    "        activation_count = 0\n",
    "        x = coords.clone().detach().requires_grad_(True)\n",
    "        activations['input'] = x\n",
    "        for i, layer in enumerate(self.net):\n",
    "            if isinstance(layer, SineLayer):\n",
    "                x, intermed = layer.forward_with_intermediate(x)\n",
    "                \n",
    "                if retain_grad:\n",
    "                    x.retain_grad()\n",
    "                    intermed.retain_grad()\n",
    "                    \n",
    "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
    "                activation_count += 1\n",
    "            else: \n",
    "                x = layer(x)\n",
    "                \n",
    "                if retain_grad:\n",
    "                    x.retain_grad()\n",
    "                    \n",
    "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
    "            activation_count += 1\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class AudioFile(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data, self.rate = torchaudio.load(filename, normalize=False)\n",
    "        self.data = self.data.transpose(-1,-2).numpy()[...,0]\n",
    "        # self.data = self.data.astype(np.float16)\n",
    "        self.timepoints = get_mgrid(len(self.data), 1)\n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.timepoints.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        amplitude = self.data\n",
    "        amplitude = (amplitude / 32768.)\n",
    "        amplitude = torch.Tensor(amplitude).view(-1, 1)\n",
    "\n",
    "        amplitude = amplitude / amplitude.abs().max()\n",
    "        return self.timepoints, amplitude\n",
    "\n",
    "def spectrogram(wav):\n",
    "    stft = torchaudio.transforms.Spectrogram(n_fft=1024)(wav)[0]\n",
    "    stft = torchaudio.transforms.AmplitudeToDB(top_db=80)(stft).numpy()\n",
    "    stft = stft[::-1]\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    im = plt.imshow(stft)\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "#############################################################################  \n",
    "\n",
    "class AudioFile4MelSpec(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data, self.rate = torchaudio.load(filename, normalize=False)\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(48000, n_fft=512)(self.data)\n",
    "        print(\"melspec shape : \", self.melspec.shape)\n",
    "        # self.data = self.data.astype(np.float16)\n",
    "        self.coordinates = get_mgrid(self.melspec, 2)\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(48000, n_fft=512)(self.data).reshape(-1,1)\n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.coordinates.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx > 0: raise IndexError\n",
    "        \n",
    "        return self.coordinates, self.melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a9a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit = torchaudio.backend.sox_io_backend.info(os.path.join(datapath, 'all_mono.wav')).bits_per_sample\n",
    "datapath = 'C:/Users/Yoon/Desktop/INR_음성'\n",
    "    \n",
    "audio, rate = torchaudio.load(os.path.join(datapath, 'all_mono_0.wav'))\n",
    "# for idx, i in enumerate(range(0, audio.shape[-1], rate * 10)):\n",
    "#     torchaudio.save(os.path.join(datapath, f'all_mono_{idx}.wav'), audio[:, i:i+rate*10], sample_rate=rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6362778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32037690",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec = torchaudio.transforms.MelSpectrogram(48000, n_fft=512)(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f6787fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240128, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48d06271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1876])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08f4759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-64.7125, -49.0485, -44.0750,  ..., -16.2432, -15.1434, -15.1447],\n",
       "        [-65.2663, -49.8303, -44.2795,  ..., -17.1337, -16.2362, -16.2685],\n",
       "        [-70.0858, -56.8328, -47.2210,  ..., -25.5872, -31.6626, -26.9972],\n",
       "        ...,\n",
       "        [-61.8620, -59.9381, -50.6104,  ..., -24.7710, -24.2369, -25.0626],\n",
       "        [-61.1269, -58.5470, -51.6667,  ..., -29.4719, -28.9445, -25.4422],\n",
       "        [-62.3545, -62.6609, -58.6694,  ..., -43.9952, -42.9630, -26.5210]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4eabef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-64.7125, -49.0485, -44.0750,  ..., -16.2432, -15.1434, -15.1447])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2c392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath2 = 'C:/Users/Yoon/Desktop/INR_음성/all_mono_0.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc20bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavpath = sorted(glob(os.path.join(datapath, f'all_mono_*.wav')), key=lambda x: int(os.path.basename(x).split('.')[0].split('_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60913d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6368c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa228e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d766851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec shape :  torch.Size([1, 128, 1876])\n",
      "coordinates shape :  torch.Size([240128, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchaudio\\functional\\functional.py:508: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  \"At least one mel filterbank has all zero values. \"\n"
     ]
    }
   ],
   "source": [
    "total_steps = 2000\n",
    "steps_til_summary = 1000\n",
    "\n",
    "pesq = []\n",
    "losses = []\n",
    "lr = 1e-4\n",
    "#for wave in wavpath:\n",
    "#bach_audio = AudioFile(wave)\n",
    "bach_audio = AudioFile4MelSpec(datapath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bffae48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240128, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bach_audio.melspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c02e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240128, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bach_audio.coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bc0d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_siren = Siren(in_features=2, out_features=1, hidden_features=256,   # time이 input일때는 in_feat = 1\n",
    "                    hidden_layers=3, first_omega_0=15000, outermost_linear=True)\n",
    "audio_siren = audio_siren.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "899ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(bach_audio, shuffle=True, batch_size=1, pin_memory=True, num_workers=0)\n",
    "\n",
    "model_input, ground_truth = next(iter(dataloader))\n",
    "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eab90483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 240128, 2]), torch.Size([1, 240128, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape, ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af035513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69217a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 2000/2000 [02:48<00:00, 11.90it/s, loss=11.8]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(lr=lr, params=audio_siren.parameters())\n",
    "#decay = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=1/2**0.5, patience=20, verbose=False)\n",
    "\n",
    "minloss = torch.inf\n",
    "with tqdm(range(total_steps)) as pbar:\n",
    "    for step in pbar:\n",
    "        optim.zero_grad()\n",
    "        model_output, coords = audio_siren(model_input)\n",
    "        #loss = singlesrc_neg_snr(model_output.squeeze(-1), ground_truth.squeeze(-1))\n",
    "        loss = F.mse_loss(model_output, ground_truth)\n",
    "\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # schedule.step(step)\n",
    "        #decay.step(loss.item())\n",
    "        if step > int(total_steps * 0.9) and minloss >= loss.item():\n",
    "            minloss = loss.item()\n",
    "            best = audio_siren.state_dict()\n",
    "\n",
    "losses.append(minloss)\n",
    "audio_siren.load_state_dict(best)\n",
    "audio_siren.eval()                  # evaluate\n",
    "with torch.no_grad():\n",
    "    model_output, _ = audio_siren(model_input)\n",
    "\n",
    "# model_output = model_output.float()\n",
    "# ground_truth = ground_truth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57a70264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 240128, 1]), torch.Size([1, 240128, 1]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.shape, ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8fcb3208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4645],\n",
       "         [0.4894],\n",
       "         [0.4916],\n",
       "         ...,\n",
       "         [0.4709],\n",
       "         [0.4883],\n",
       "         [0.4648]]], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fac5ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_melspec = torch.reshape(model_output, (1, 128, 1876))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9dc53d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1876])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_melspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6ddc561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.6602e-07, 3.2068e-07, 4.3920e-07,  ..., 1.0961e-07,\n",
       "          5.2605e-07, 6.7005e-04],\n",
       "         [9.9279e-08, 3.2986e-07, 4.2404e-07,  ..., 1.4116e-07,\n",
       "          2.7236e-07, 6.4258e-04],\n",
       "         [1.4521e-07, 3.3054e-07, 2.9702e-07,  ..., 1.8400e-07,\n",
       "          1.2756e-07, 6.3113e-04]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6388ea10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4645, 0.4894, 0.4916,  ..., 0.4893, 0.4998, 0.4613],\n",
       "         [0.4918, 0.4440, 0.4982,  ..., 0.4982, 0.5147, 0.4714],\n",
       "         [0.4639, 0.4801, 0.4656,  ..., 0.4589, 0.5173, 0.4830],\n",
       "         ...,\n",
       "         [0.4924, 0.4791, 0.5008,  ..., 0.4905, 0.4966, 0.4493],\n",
       "         [0.4979, 0.4851, 0.4853,  ..., 0.4915, 0.4738, 0.4716],\n",
       "         [0.4689, 0.4876, 0.4954,  ..., 0.4709, 0.4883, 0.4648]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9a3f743b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00],\n",
       "         [0.0000e+00],\n",
       "         [0.0000e+00],\n",
       "         ...,\n",
       "         [1.8400e-07],\n",
       "         [1.2756e-07],\n",
       "         [6.3113e-04]]], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "464ce92d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3544\\2115167185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_melspec\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmelspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"match\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "if(recon_melspec == melspec):\n",
    "    print(\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad6487cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_melspec.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a493d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse stft\n",
    "#recon_melspec = torchaudio.functional.DB_to_amplitude(recon_melspec, ref=1, power=0.5)\n",
    "inverse_melspec = torchaudio.transforms.InverseMelScale(512, sample_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7ab02112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3544\\1247886785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_melspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_melspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchaudio\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, melspec)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelspec\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspecgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0mnew_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;31m# take sum over mel-frequency then average over other dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "transform = inverse_melspec(recon_melspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8ada9cae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "istft input and window must be on the same device but got self on cuda:0 and window on cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3544\\1388212556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGriffinLim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_melspec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m257\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchaudio\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, specgram)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         )\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchaudio\\functional\\functional.py\u001b[0m in \u001b[0;36mgriffinlim\u001b[1;34m(specgram, window, n_fft, hop_length, win_length, power, n_iter, momentum, length, rand_init)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;31m# Invert with our current estimate of the phases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         inverse = torch.istft(\n\u001b[1;32m--> 265\u001b[1;33m             \u001b[0mspecgram\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mangles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mistft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m     return _VF.istft(input, n_fft, hop_length, win_length, window, center,  # type: ignore[attr-defined]\n\u001b[1;32m--> 771\u001b[1;33m                      normalized, onesided, length, return_complex)\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: istft input and window must be on the same device but got self on cuda:0 and window on cpu"
     ]
    }
   ],
   "source": [
    "transform = torchaudio.transforms.GriffinLim(n_fft=512, hop_length=256)(recon_melspec[:,:257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aee6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa를 활용한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3551a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "\n",
    "recon_melspec_numpy = recon_melspec.cpu().detach().numpy()\n",
    "recon_melspec_numpy = librosa.db_to_amplitude(recon_melspec_numpy)\n",
    "S = librosa.feature.inverse.mel_to_audio(recon_melspec_numpy, n_fft=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32c77713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 1876)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_melspec_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c81c563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 240000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5256340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3388167 ,  0.32235208,  1.1411444 , ...,  0.24876854,\n",
       "        -0.05224194,  0.3675938 ]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ced7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t = S.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a3c862a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3fe92533",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t = S_t.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "91ed0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0fe580d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (257,240000) into shape (1,240000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3544\\3120796394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgriffinlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\util\\decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\core\\spectrum.py\u001b[0m in \u001b[0;36mgriffinlim\u001b[1;34m(S, n_iter, hop_length, win_length, n_fft, window, center, dtype, length, pad_mode, momentum, init, random_state)\u001b[0m\n\u001b[0;32m   2458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2459\u001b[0m         \u001b[1;31m# Update our phase estimates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2460\u001b[1;33m         \u001b[0mangles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuilt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtprev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2461\u001b[0m         \u001b[0mangles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (257,240000) into shape (1,240000)"
     ]
    }
   ],
   "source": [
    "y = librosa.griffinlim(S, n_fft=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73831463",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfile.write(\"C:/Users/Yoon/Desktop/recon.wav\", S_t, samplerate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(os.path.join(datapath, '15000_SNR_recons_' + os.path.basename(wave)), model_output.squeeze(-1).cpu().float(), sample_rate=rate)\n",
    "model_output = torchaudio.functional.resample(model_output.squeeze(-1), rate, 16000).squeeze().cpu()\n",
    "model_input = torchaudio.functional.resample(model_input.squeeze(-1), rate, 16000).squeeze().cpu()\n",
    "\n",
    "# model_output, _ = torchaudio.load(os.path.join(datapath, 'recons' + os.path.basename(wave)))\n",
    "model_output = model_output.squeeze().numpy()\n",
    "ground_truth = torchaudio.functional.resample(ground_truth.squeeze(-1), rate, 16000).squeeze().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb08364",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq.append(get_metrics(model_output, ground_truth.numpy(), model_output, sample_rate=16000, metrics_list=['pesq'])['pesq'])\n",
    "print(pesq[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5235735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(pesq), min(pesq), np.mean(pesq))\n",
    "plt.scatter(np.arange(len(pesq)), pesq)\n",
    "plt.plot(np.ones_like(pesq) * 3)\n",
    "plt.plot(np.ones_like(pesq) * 4)\n",
    "print(pesq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783925e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e66150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c002a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
